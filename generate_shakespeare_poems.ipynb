{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be43848d-2fdf-43f9-8040-c94168b78afe",
   "metadata": {},
   "source": [
    "# Sequence Generation : Shakespeare Poems with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9490a6b-4b66-41a6-b148-de8c472d54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input,Dropout\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93c2b0b-8148-4113-ac8e-f2f6c0c68237",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=open(\"shakespeare.txt\",'r').read()\n",
    "vocab=sorted(set(text))\n",
    "char2idx={ch:i for i,ch in enumerate(vocab)}\n",
    "idx2char={i:ch for i,ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b36db53-cc0b-4b3e-9a43-dc9f3e5c73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text,tx=40,stride=3):\n",
    "    '''\n",
    "    Divides input text to X and Y list\n",
    "    \n",
    "    Arguments:\n",
    "    text -- text data (str)\n",
    "    tx   -- no. of timesteps (int)\n",
    "    stride--move pointer according to stride (int)\n",
    "    \n",
    "    Returns:\n",
    "    X -- list in with each item is text of length tx\n",
    "    Y -- list of expected output or each item in X\n",
    "    \n",
    "    '''\n",
    "    X,Y=[],[]\n",
    "    for i in range(0,len(text)-tx,stride):\n",
    "        X.append(text[i:i+tx])\n",
    "        Y.append(text[i+tx])\n",
    "    \n",
    "    print(\"Total training examples : \",len(X))\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5adab9-3316-487c-a052-7c4e520eeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(X,Y,n_x,char2idx,tx=40):\n",
    "    '''\n",
    "    Vectorization of training data. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- list in with each item is text of length tx\n",
    "    Y -- list of expected output or each item in X\n",
    "    n_x -- vocab size (int)\n",
    "    char2idx -- dictionary with char:index pairs\n",
    "    tx -- no. of timesteps (int)\n",
    "    \n",
    "    Returns:\n",
    "    x -- 3D array of size (no. of training examples,tx,n_x)\n",
    "    y -- 2D array of size (no. of training examples,n_x)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m=len(X)\n",
    "    x=np.zeros((m,tx,n_x))\n",
    "    y=np.zeros((m,n_x))\n",
    "    \n",
    "    for i,sentence in enumerate(X):\n",
    "        for t,char in enumerate(sentence):\n",
    "            x[i,t,char2idx[char]]=1\n",
    "        y[i,char2idx[Y[i]]]=1\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74dc87d-f9c4-4f40-a5b7-61d63241ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_x,tx=40):\n",
    "    '''\n",
    "    Builds model\n",
    "    Arguments:\n",
    "    n_x -- vocab size (int)\n",
    "    tx -- no. of timesteps (int)\n",
    "    \n",
    "    Returns:\n",
    "    model -- tensorflow model\n",
    "    \n",
    "    '''\n",
    "    inputs=Input(shape=(tx,n_x))\n",
    "    #lstm_out1=LSTM(128,return_sequences=True)(inputs)\n",
    "    #dropout1=Dropout(0.2)(lstm_out1)\n",
    "    lstm_out2=LSTM(128)(inputs)\n",
    "    dropout2=Dropout(0.2)(lstm_out2)\n",
    "    outputs=Dense(n_x,activation=\"softmax\")(dropout2)\n",
    "    model=Model(inputs=inputs,outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db51821-f475-4e34-967a-1218216dfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def generate_output(model,tx,n_x,char2idx,output_chars=100):\n",
    "    '''\n",
    "    Generates output \n",
    "    Arguments:\n",
    "    model -- tensorflow model\n",
    "    n_x -- vocab size (int)\n",
    "    tx -- no. of timesteps (int)\n",
    "    char2idx -- dictionary with char:index pairs\n",
    "    output_chars -- no. of characters to generate (int)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    generated=''\n",
    "    user_input=input(\"Your text\")\n",
    "    sentence=sentence='0'*(tx-len(user_input))+user_input\n",
    "    generated+=user_input\n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
    "    #sys.stdout.write(usr_input)\n",
    "    print(user_input,end='')\n",
    "    \n",
    "    for i in range(output_chars):\n",
    "        x=np.zeros((1,tx,n_x))\n",
    "        for t,ch in enumerate(sentence):\n",
    "            if ch!='0':\n",
    "                x[0,t,char2idx[ch]]=1\n",
    "        \n",
    "        y_pred=model(x) #returns softmax probability of every word in vocab\n",
    "        #print(\"sum\",tf.reduce_sum(y_pred))\n",
    "        sampled_idx=np.random.choice(range(len(vocab)),p=np.squeeze(np.array(y_pred))) #sample by probability to generate new sequence\n",
    "        nextchar=idx2char[sampled_idx]\n",
    "        \n",
    "        generated+=nextchar\n",
    "        sentence=sentence[1:]+nextchar\n",
    "        \n",
    "        sys.stdout.write(nextchar)\n",
    "        #sys.stdout.flush()\n",
    "    #print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9741a99f-2452-46cf-992b-d26a814fb328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples :  31412\n"
     ]
    }
   ],
   "source": [
    "n_x=len(vocab) #vocab size\n",
    "tx=40 #total timesteps\n",
    "stride=3\n",
    "X,Y=build_data(text,tx,stride)\n",
    "x,y=vectorization(X,Y,n_x,char2idx,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77067db9-6e55-447f-90b1-2367baca8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 11:36:00.219849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-21 11:36:00.220784: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-21 11:36:00.220871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7655cbfd5bcd): /proc/driver/nvidia/version does not exist\n",
      "2023-01-21 11:36:00.222673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 61)]          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               97280     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 61)                7869      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,149\n",
      "Trainable params: 105,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=build_model(n_x,tx)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce5b2e9-8d36-4566-85fc-74dc77e61f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves models in every 5 epochs\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch%5 == 0:  # or save after some epoch, each k-th epoch etc.\n",
    "            self.model.save(\"models/my_shakespeare_model/my_shakespeare_model_epoch_{}.h5\".format(epoch))\n",
    "\n",
    "saver=CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efb332-5842-4982-bc95-8921939ca6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "491/491 [==============================] - 44s 80ms/step - loss: 2.8162 - accuracy: 0.2375\n",
      "Epoch 2/350\n",
      "491/491 [==============================] - 41s 82ms/step - loss: 2.3965 - accuracy: 0.3242\n",
      "Epoch 3/350\n",
      "491/491 [==============================] - 47s 95ms/step - loss: 2.2484 - accuracy: 0.3541\n",
      "Epoch 4/350\n",
      "491/491 [==============================] - 47s 96ms/step - loss: 2.1528 - accuracy: 0.3777\n",
      "Epoch 5/350\n",
      "491/491 [==============================] - 47s 95ms/step - loss: 2.0876 - accuracy: 0.3967\n",
      "Epoch 6/350\n",
      "491/491 [==============================] - 47s 96ms/step - loss: 2.0341 - accuracy: 0.4087\n",
      "Epoch 7/350\n",
      "491/491 [==============================] - 47s 96ms/step - loss: 1.9899 - accuracy: 0.4180\n",
      "Epoch 8/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.9505 - accuracy: 0.4273\n",
      "Epoch 9/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.9158 - accuracy: 0.4353\n",
      "Epoch 10/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.8841 - accuracy: 0.4415\n",
      "Epoch 11/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.8525 - accuracy: 0.4480\n",
      "Epoch 12/350\n",
      "491/491 [==============================] - 49s 99ms/step - loss: 1.8298 - accuracy: 0.4534\n",
      "Epoch 13/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 1.8036 - accuracy: 0.4592\n",
      "Epoch 14/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 1.7778 - accuracy: 0.4684\n",
      "Epoch 15/350\n",
      "491/491 [==============================] - 49s 99ms/step - loss: 1.7495 - accuracy: 0.4762\n",
      "Epoch 16/350\n",
      "491/491 [==============================] - 49s 99ms/step - loss: 1.7317 - accuracy: 0.4796\n",
      "Epoch 17/350\n",
      "491/491 [==============================] - 48s 98ms/step - loss: 1.7105 - accuracy: 0.4863\n",
      "Epoch 18/350\n",
      "491/491 [==============================] - 45s 92ms/step - loss: 1.6861 - accuracy: 0.4910\n",
      "Epoch 19/350\n",
      "491/491 [==============================] - 47s 96ms/step - loss: 1.6703 - accuracy: 0.4955\n",
      "Epoch 20/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 1.6485 - accuracy: 0.4998\n",
      "Epoch 21/350\n",
      "491/491 [==============================] - 49s 99ms/step - loss: 1.6224 - accuracy: 0.5076\n",
      "Epoch 22/350\n",
      "491/491 [==============================] - 45s 92ms/step - loss: 1.6055 - accuracy: 0.5105\n",
      "Epoch 23/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.5875 - accuracy: 0.5140\n",
      "Epoch 24/350\n",
      "491/491 [==============================] - 49s 99ms/step - loss: 1.5668 - accuracy: 0.5210\n",
      "Epoch 25/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.5457 - accuracy: 0.5271\n",
      "Epoch 26/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.5212 - accuracy: 0.5353\n",
      "Epoch 27/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.5040 - accuracy: 0.5372\n",
      "Epoch 28/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.4842 - accuracy: 0.5436\n",
      "Epoch 29/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.4621 - accuracy: 0.5508\n",
      "Epoch 30/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 1.4397 - accuracy: 0.5554\n",
      "Epoch 31/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 1.4264 - accuracy: 0.5612\n",
      "Epoch 32/350\n",
      "491/491 [==============================] - 48s 98ms/step - loss: 1.3931 - accuracy: 0.5689\n",
      "Epoch 33/350\n",
      "491/491 [==============================] - 48s 97ms/step - loss: 1.3773 - accuracy: 0.5741\n",
      "Epoch 34/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.3531 - accuracy: 0.5789\n",
      "Epoch 35/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 1.3329 - accuracy: 0.5860\n",
      "Epoch 36/350\n",
      "491/491 [==============================] - 49s 101ms/step - loss: 1.3140 - accuracy: 0.5894\n",
      "Epoch 37/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 1.2913 - accuracy: 0.5966\n",
      "Epoch 38/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 1.2723 - accuracy: 0.6036\n",
      "Epoch 39/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.2551 - accuracy: 0.6078\n",
      "Epoch 40/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.2308 - accuracy: 0.6138\n",
      "Epoch 41/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.2096 - accuracy: 0.6217\n",
      "Epoch 42/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 1.1890 - accuracy: 0.6278\n",
      "Epoch 43/350\n",
      "491/491 [==============================] - 49s 101ms/step - loss: 1.1696 - accuracy: 0.6340\n",
      "Epoch 44/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.1526 - accuracy: 0.6364\n",
      "Epoch 45/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.1375 - accuracy: 0.6437\n",
      "Epoch 46/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.1186 - accuracy: 0.6466\n",
      "Epoch 47/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0978 - accuracy: 0.6533\n",
      "Epoch 48/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0893 - accuracy: 0.6551\n",
      "Epoch 49/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0667 - accuracy: 0.6633\n",
      "Epoch 50/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0490 - accuracy: 0.6675\n",
      "Epoch 51/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 1.0397 - accuracy: 0.6696\n",
      "Epoch 52/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0233 - accuracy: 0.6739\n",
      "Epoch 53/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 1.0124 - accuracy: 0.6787\n",
      "Epoch 54/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.9977 - accuracy: 0.6824\n",
      "Epoch 55/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.9788 - accuracy: 0.6902\n",
      "Epoch 56/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 0.9745 - accuracy: 0.6886\n",
      "Epoch 57/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 0.9627 - accuracy: 0.6932\n",
      "Epoch 58/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.9513 - accuracy: 0.6957\n",
      "Epoch 59/350\n",
      "491/491 [==============================] - 52s 106ms/step - loss: 0.9373 - accuracy: 0.6998\n",
      "Epoch 60/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.9278 - accuracy: 0.7049\n",
      "Epoch 61/350\n",
      "491/491 [==============================] - 52s 105ms/step - loss: 0.9127 - accuracy: 0.7064\n",
      "Epoch 62/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.9158 - accuracy: 0.7098\n",
      "Epoch 63/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.9090 - accuracy: 0.7076\n",
      "Epoch 64/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.9010 - accuracy: 0.7111\n",
      "Epoch 65/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8824 - accuracy: 0.7164\n",
      "Epoch 66/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8735 - accuracy: 0.7203\n",
      "Epoch 67/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.8716 - accuracy: 0.7209\n",
      "Epoch 68/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8515 - accuracy: 0.7270\n",
      "Epoch 69/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8565 - accuracy: 0.7260\n",
      "Epoch 70/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.8507 - accuracy: 0.7270\n",
      "Epoch 71/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.8420 - accuracy: 0.7295\n",
      "Epoch 72/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.8329 - accuracy: 0.7322\n",
      "Epoch 73/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8280 - accuracy: 0.7338\n",
      "Epoch 74/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.8150 - accuracy: 0.7374\n",
      "Epoch 75/350\n",
      "491/491 [==============================] - 52s 105ms/step - loss: 0.8207 - accuracy: 0.7368\n",
      "Epoch 76/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.8153 - accuracy: 0.7379\n",
      "Epoch 77/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7993 - accuracy: 0.7421\n",
      "Epoch 78/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7920 - accuracy: 0.7438\n",
      "Epoch 79/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7890 - accuracy: 0.7446\n",
      "Epoch 80/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7838 - accuracy: 0.7465\n",
      "Epoch 81/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.7891 - accuracy: 0.7443\n",
      "Epoch 82/350\n",
      "491/491 [==============================] - 49s 101ms/step - loss: 0.7922 - accuracy: 0.7442\n",
      "Epoch 83/350\n",
      "491/491 [==============================] - 49s 101ms/step - loss: 0.7838 - accuracy: 0.7446\n",
      "Epoch 84/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7789 - accuracy: 0.7477\n",
      "Epoch 85/350\n",
      "491/491 [==============================] - 52s 106ms/step - loss: 0.7619 - accuracy: 0.7529\n",
      "Epoch 86/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.7547 - accuracy: 0.7561\n",
      "Epoch 87/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7557 - accuracy: 0.7545\n",
      "Epoch 88/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7565 - accuracy: 0.7525\n",
      "Epoch 89/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.7546 - accuracy: 0.7563\n",
      "Epoch 90/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.7535 - accuracy: 0.7554\n",
      "Epoch 91/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7477 - accuracy: 0.7582\n",
      "Epoch 92/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7312 - accuracy: 0.7597\n",
      "Epoch 93/350\n",
      "491/491 [==============================] - 49s 100ms/step - loss: 0.7401 - accuracy: 0.7593\n",
      "Epoch 94/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7293 - accuracy: 0.7616\n",
      "Epoch 95/350\n",
      "491/491 [==============================] - 52s 105ms/step - loss: 0.7316 - accuracy: 0.7637\n",
      "Epoch 96/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7256 - accuracy: 0.7628\n",
      "Epoch 97/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7313 - accuracy: 0.7601\n",
      "Epoch 98/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.7227 - accuracy: 0.7647\n",
      "Epoch 99/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7151 - accuracy: 0.7661\n",
      "Epoch 100/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.7126 - accuracy: 0.7660\n",
      "Epoch 101/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7040 - accuracy: 0.7711\n",
      "Epoch 102/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.7055 - accuracy: 0.7696\n",
      "Epoch 103/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6979 - accuracy: 0.7728\n",
      "Epoch 104/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6982 - accuracy: 0.7720\n",
      "Epoch 105/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6976 - accuracy: 0.7718\n",
      "Epoch 106/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.7018 - accuracy: 0.7707\n",
      "Epoch 107/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.7014 - accuracy: 0.7691\n",
      "Epoch 108/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.6974 - accuracy: 0.7694\n",
      "Epoch 109/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6951 - accuracy: 0.7718\n",
      "Epoch 110/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.6911 - accuracy: 0.7723\n",
      "Epoch 111/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6885 - accuracy: 0.7732\n",
      "Epoch 112/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6759 - accuracy: 0.7762\n",
      "Epoch 113/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6774 - accuracy: 0.7783\n",
      "Epoch 114/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.6790 - accuracy: 0.7773\n",
      "Epoch 115/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6806 - accuracy: 0.7753\n",
      "Epoch 116/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6767 - accuracy: 0.7798\n",
      "Epoch 117/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6785 - accuracy: 0.7766\n",
      "Epoch 118/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6593 - accuracy: 0.7822\n",
      "Epoch 119/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6580 - accuracy: 0.7837\n",
      "Epoch 120/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6662 - accuracy: 0.7800\n",
      "Epoch 121/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.6592 - accuracy: 0.7840\n",
      "Epoch 122/350\n",
      "491/491 [==============================] - 53s 108ms/step - loss: 0.6659 - accuracy: 0.7813\n",
      "Epoch 123/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6690 - accuracy: 0.7791\n",
      "Epoch 124/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6535 - accuracy: 0.7861\n",
      "Epoch 125/350\n",
      "491/491 [==============================] - 50s 102ms/step - loss: 0.6489 - accuracy: 0.7879\n",
      "Epoch 126/350\n",
      "491/491 [==============================] - 51s 103ms/step - loss: 0.6378 - accuracy: 0.7907\n",
      "Epoch 127/350\n",
      "491/491 [==============================] - 51s 105ms/step - loss: 0.6558 - accuracy: 0.7845\n",
      "Epoch 128/350\n",
      "491/491 [==============================] - 50s 103ms/step - loss: 0.6375 - accuracy: 0.7889\n",
      "Epoch 129/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6421 - accuracy: 0.7876\n",
      "Epoch 130/350\n",
      "491/491 [==============================] - 51s 104ms/step - loss: 0.6296 - accuracy: 0.7940\n",
      "Epoch 131/350\n",
      "491/491 [==============================] - 50s 101ms/step - loss: 0.6424 - accuracy: 0.7893\n",
      "Epoch 132/350\n",
      "491/491 [==============================] - 268s 547ms/step - loss: 0.6480 - accuracy: 0.7866\n",
      "Epoch 133/350\n",
      "  3/491 [..............................] - ETA: 21:57:45 - loss: 0.5834 - accuracy: 0.8125"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "history=model.fit(x,y,batch_size=64,epochs=350,callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e61815e-d3fb-44cd-82a3-30fef68aa1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 13:56:53.452220: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-21 13:56:53.453535: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-21 13:56:53.453746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7655cbfd5bcd): /proc/driver/nvidia/version does not exist\n",
      "2023-01-21 13:56:53.456183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#latest saved model is model with epoch 130, so load it \n",
    "saved_model=load_model(\"models/my_shakespeare_model/my_shakespeare_model_epoch_130.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a392a8-8635-49a1-afd2-1d120330bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your text thou evil shall not rise \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "thou evil shall not rise incherss,\n",
      "But me true by to pice a livet deaish,\n",
      "And my desprade ade make If far my me pit:\n",
      "And sun doth bll to sue small sight suale from cheer\n",
      "Adcinlt heart's breath henounoon Maines bling meathe, tree,\n",
      "As prife heaves of the wind compare chore,\n",
      "My seavere on thin times of bery will:\n",
      "Mo eyes sweet in in still I senter dith\n",
      "To slowe, bornce the be wothing to me day\n",
      "What ever surmed, huwbless not "
     ]
    }
   ],
   "source": [
    "output_chars=400\n",
    "generate_output(saved_model,tx,n_x,char2idx,output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60444a0d-d61f-4b3d-bc42-b612fc3a7b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
